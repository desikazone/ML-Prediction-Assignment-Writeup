{"name":"Ml-prediction-assignment-writeup","tagline":"","body":"# Predictive Model to determine the exercising pattern\r\nThis document describe the analysis done for the prediction assignment of the practical machine learning course.\r\n\r\n1.The first step is to load the csv file data to dataframe\r\n```{r}\r\n  pml_training<-read.csv(\"C:/Ebooks/R/coursera/Machine learning/Raw data/pml-training.csv\")\r\n  pml_testing<-read.csv(\"C:/Ebooks/R/coursera/Machine learning/Raw data/pml-testing.csv\")\r\n```\r\n\r\n2.After loading the files, I split the pml_training data into test and training data so that I can cross validate the results/output.Pml_testing data is left untouched till finalization of the model.I have split the data such that 75% of the data is classified as training and rest as testing.\r\n```{r}\r\n  library(caret)\r\n  library(kernlab)\r\n  intrain<-createDataPartition(pml_training$classe,p=.75,list=FALSE)\r\n  training<-pml_training[intrain,]\r\n  test<-pml_training[-intrain,]\r\n```\r\n3.Next is the data cleaning stage.\r\n\r\n  a. First the column x present in the data is removed since it is just an index and will not be helpful for the analysis\r\n```{r}\r\n  ###remove column x which is just the index###\r\n  train_cln1<-training[,-1]\r\n```\r\n  b. Second remove the columns with more than 60% NAs. Columns with more than 60% NAs will not be good enough to contribute to the predictive model.\r\n```{r}\r\n  ###Removing variables with more than 60% NAs####\r\n  remove_var<-rep(NA,1)#create a vector with length 1 containing value NA\r\n  temp      <-vector('character') #empty vectory\r\n  for (i in 1:length(train_cln1))\r\n  {\r\n   if (sum(is.na(train_cln1[i]))/nrow(train_cln1[i]) >=.6) temp<-colnames(train_cln1[i])#get colname if NAs >=.6\r\n   if (length(temp)==1) remove_var<-unique(rbind(remove_var,temp))#collate all colnames with NAs>=.6\r\n  }\r\n  remove_var     <-as.vector(remove_var)#convert into vector\r\n  varNA          <-names(train_cln1) %in% remove_var #finalizing columns with NAs >=.6\r\n  train_cln2     <-train_cln1[!varNA] #dataset where columns with NAs>=.6 removed\r\n```\r\n c. Next step is removal of near zero variance variables.Datasets sometimes contain variables which contain almost constant values throughout the data. These are non  informative and will not add any value to the model building process.\r\n ```{r}\r\n  ####remove Near Zero Variance variables##########\r\n  dataNZV        <- nearZeroVar(train_cln2, saveMetrics=TRUE)#function  to find near zero variance variables\r\n  NZVvar         <-as.vector(row.names(dataNZV[dataNZV$nzv==\"TRUE\",]))#obtain column names of near zero variance variables\r\n  NZVvar_bin     <-names(train_cln2) %in% NZVvar #match and pick the final near zero variance variables\r\n  train_cln3     <-train_cln2[!NZVvar_bin] #remove the NZV from training data\r\n  ```\r\n d. Final step is to clean training, validation and test datasets for removal of variables identified as non-important based on above 3 steps\r\n ```{r}\r\n #########cleaning both validation and test data sets########\r\ncln1           <-names(train_cln3)#column names to be kept in training data\r\ncln2           <-names(train_cln3[-58])#column names to be kept in testing data\r\ntest           <-test[cln1]#keep only column names in cln1 vector\r\npml_testing1    <-pml_testing[cln2]#keep column names present in cln2 vector\r\nlevels(pml_testing1$cvtd_timestamp)<-levels(train_cln3$cvtd_timestamp)#this step is included since R throws an error while predicting due to difference in levels of training and test data\r\n```\r\n4.Next stage is model building. Two models are build one is prediction with Decision tree and the other with Random Forest. Confusion matrix is obtained to check the accuracy of both the models built. Model with high accuracy is shortlisted for the final prediction.\r\n```{r}\r\n##############Prediction using Decision Tree############\r\ndtree_fit<-rpart(classe~.,method=\"class\",data=train_cln3)\r\nlibrary(rattle)\r\nfancyRpartPlot(dtree_fit)\r\ndtree_predict<-predict(dtree_fit,test,type=\"class\")\r\nconfusionMatrix(dtree_predict,test$classe)\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1346   44    4    1    0\r\n         B   29  785   55   44    0\r\n         C   20  117  783  119   38\r\n         D    0    3    5  508   34\r\n         E    0    0    8  132  829\r\n\r\nOverall Statistics\r\n                                         \r\n               Accuracy : 0.8668         \r\n                 95% CI : (0.857, 0.8762)\r\n    No Information Rate : 0.2845         \r\n    P-Value [Acc > NIR] : < 2.2e-16      \r\n                                         \r\n                  Kappa : 0.8315         \r\n Mcnemar's Test P-Value : NA             \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9649   0.8272   0.9158   0.6318   0.9201\r\nSpecificity            0.9860   0.9676   0.9274   0.9898   0.9650\r\nPos Pred Value         0.9649   0.8598   0.7270   0.9236   0.8555\r\nNeg Pred Value         0.9860   0.9589   0.9812   0.9320   0.9817\r\nPrevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\nDetection Rate         0.2745   0.1601   0.1597   0.1036   0.1690\r\nDetection Prevalence   0.2845   0.1862   0.2196   0.1122   0.1976\r\nBalanced Accuracy      0.9755   0.8974   0.9216   0.8108   0.9426\r\n\r\n###########Predictino using random forest###########\r\nrf_fit<-randomForest(classe~.,data=train_cln3)\r\nrf_predict<-predict(rf_fit,test,type=\"class\")\r\nconfusionMatrix(rf_predict,test$classe)\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1395    0    0    0    0\r\n         B    0  949    2    0    0\r\n         C    0    0  850    1    0\r\n         D    0    0    3  802    0\r\n         E    0    0    0    1  901\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9986          \r\n                 95% CI : (0.9971, 0.9994)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9982          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            1.0000   1.0000   0.9942   0.9975   1.0000\r\nSpecificity            1.0000   0.9995   0.9998   0.9993   0.9998\r\nPos Pred Value         1.0000   0.9979   0.9988   0.9963   0.9989\r\nNeg Pred Value         1.0000   1.0000   0.9988   0.9995   1.0000\r\nPrevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\nDetection Rate         0.2845   0.1935   0.1733   0.1635   0.1837\r\nDetection Prevalence   0.2845   0.1939   0.1735   0.1642   0.1839\r\nBalanced Accuracy      1.0000   0.9997   0.9970   0.9984   0.9999\r\n```\r\n5.Obtain the predictions by applying the model on test data.Since random forest method gives better accuracy it is finalized and applied on the test data to obtain the predictions\r\n```{r}\r\n#######since random forest method gives better accuracy using that on test data####\r\nrf_predict1<-predict(rf_fit,pml_testing1,type=\"class\")\r\ncbind(problem_id=pml_testing$problem_id,as.data.frame(rf_predict1))\r\n   problem_id rf_predict1\r\n1           1           B\r\n2           2           A\r\n3           3           B\r\n4           4           A\r\n5           5           A\r\n6           6           E\r\n7           7           D\r\n8           8           B\r\n9           9           A\r\n10         10           A\r\n11         11           B\r\n12         12           C\r\n13         13           B\r\n14         14           A\r\n15         15           E\r\n16         16           E\r\n17         17           A\r\n18         18           B\r\n19         19           B\r\n20         20           B\r\n```\r\n6.Results obtained were uploaded to the coursera assignment submission gateway for online evaluation.\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}